{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "solar-demo-rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIJDVe9N6kq2",
        "outputId": "d22b0ecb-2161-4a37-fae5-037939969141"
      },
      "source": [
        "!wget https://github.com/doctorai-in/solar-energy-prediction/raw/main/datasets.zip"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-16 07:11:32--  https://github.com/doctorai-in/solar-energy-prediction/raw/main/datasets.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/doctorai-in/solar-energy-prediction/main/datasets.zip [following]\n",
            "--2021-01-16 07:11:32--  https://raw.githubusercontent.com/doctorai-in/solar-energy-prediction/main/datasets.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152849 (149K) [application/zip]\n",
            "Saving to: ‘datasets.zip.1’\n",
            "\n",
            "datasets.zip.1      100%[===================>] 149.27K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-01-16 07:11:32 (4.67 MB/s) - ‘datasets.zip.1’ saved [152849/152849]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CBswq0p6kq-",
        "outputId": "f71ea1ee-1270-49cd-ac87-f9c7e8ee5347"
      },
      "source": [
        "!unzip datasets.zip "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  datasets.zip\n",
            "replace datasets/hourly/weather_train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: datasets/hourly/weather_train.csv  \n",
            "  inflating: datasets/hourly/weather_test.csv  \n",
            "  inflating: datasets/hourly/weather_dev.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "593-nlLb6kq_"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib as mpl\n",
        "from matplotlib.image import imread\n",
        "from random import randint\n",
        "\n",
        "import theano\n",
        "import keras\n",
        "import pandas\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras import optimizers\n",
        "import keras.utils\n",
        "import keras.layers\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import r2_score\n",
        "import copy\n",
        "import csv"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwjSnaOZ6krA"
      },
      "source": [
        "mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATASET_PATH = \"./datasets/hourly/\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQkt2nuC6krA"
      },
      "source": [
        "#Set y values of data to lie between 0 and 1\n",
        "def normalize_data(dataset, data_min, data_max):\n",
        "    data_std = (dataset - data_min) / (data_max - data_min)\n",
        "    test_scaled = data_std * (np.amax(data_std) - np.amin(data_std)) + np.amin(data_std)\n",
        "    return test_scaled\n",
        "\n",
        "#Import and pre-process data for future applications\n",
        "def import_data(train_dataframe, dev_dataframe, test_dataframe):\n",
        "    \n",
        "    dataset = train_dataframe.values\n",
        "    dataset = dataset.astype('float32')\n",
        "\n",
        "    #Include all 12 initial factors (Year ; Month ; Hour ; Day ; Cloud Coverage ; Visibility ; Temperature ; Dew Point ;\n",
        "    #Relative Humidity ; Wind Speed ; Station Pressure ; Altimeter\n",
        "    max_test = np.max(dataset[:,12])\n",
        "    min_test = np.min(dataset[:,12])\n",
        "    scale_factor = max_test - min_test\n",
        "    max = np.empty(13)\n",
        "    min = np.empty(13)\n",
        "\n",
        "    #Create training dataset\n",
        "    for i in range(0,13):\n",
        "        min[i] = np.amin(dataset[:,i],axis = 0)\n",
        "        max[i] = np.amax(dataset[:,i],axis = 0)\n",
        "        dataset[:,i] = normalize_data(dataset[:, i], min[i], max[i])\n",
        "\n",
        "    train_data = dataset[:,0:12]\n",
        "    train_labels = dataset[:,12]\n",
        "\n",
        "    # Create dev dataset\n",
        "    dataset = dev_dataframe.values\n",
        "    dataset = dataset.astype('float32')\n",
        "\n",
        "    for i in range(0, 13):\n",
        "        dataset[:, i] = normalize_data(dataset[:, i], min[i], max[i])\n",
        "\n",
        "    dev_data = dataset[:,0:12]\n",
        "    dev_labels = dataset[:,12]\n",
        "\n",
        "    # Create test dataset\n",
        "    dataset = test_dataframe.values\n",
        "    dataset = dataset.astype('float32')\n",
        "\n",
        "    for i in range(0, 13):\n",
        "        dataset[:, i] = normalize_data(dataset[:, i], min[i], max[i])\n",
        "\n",
        "    test_data = dataset[:, 0:12]\n",
        "    test_labels = dataset[:, 12]\n",
        "\n",
        "    return train_data, train_labels, dev_data, dev_labels, test_data, test_labels, scale_factor"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9AGl43i6krB"
      },
      "source": [
        "#Construt and return Keras RNN model\n",
        "def build_model(init_type = 'glorot_uniform', optimizer = 'adam', num_features = 12):\n",
        "    model = Sequential()\n",
        "    layers = [num_features, 64, 64, 1, 1]\n",
        "    model.add(keras.layers.LSTM(\n",
        "        layers[0],\n",
        "        input_shape = (None, num_features),\n",
        "        return_sequences=True))\n",
        "    model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "    model.add(keras.layers.LSTM(\n",
        "        layers[1],\n",
        "        kernel_initializer = init_type,\n",
        "        return_sequences=True\n",
        "        #bias_initializer = 'zeros'\n",
        "    ))\n",
        "    model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(\n",
        "        layers[2], activation='tanh',\n",
        "        kernel_initializer=init_type,\n",
        "        input_shape = (None, 1)\n",
        "        ))\n",
        "    model.add(Dense(\n",
        "        layers[3]))\n",
        "\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    #Alternative parameters:\n",
        "    #momentum = 0.8\n",
        "    #learning_rate = 0.1\n",
        "    #epochs = 100\n",
        "    #decay_rate = learning_rate / 100\n",
        "    #sgd = keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "    #model.compile(loss=\"binary_crossentropy\", optimizer=sgd)\n",
        "    rms = keras.optimizers.RMSprop(lr=0.002, rho=0.9, epsilon=1e-08, decay=0.01)\n",
        "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paykhXbC6krC"
      },
      "source": [
        "#Save output predictions for graphing and inspection\n",
        "def write_to_csv(prediction, filename):\n",
        "    print(\"Writing to CSV...\")\n",
        "    with open(filename, 'w') as file:\n",
        "        for i in range(prediction.shape[0]):\n",
        "            file.write(\"%.5f\" % prediction[i][0][0])\n",
        "            file.write('\\n')\n",
        "    print(\"...finished!\")\n",
        "\n",
        "# Return MSE error values of all three data sets based on a single model\n",
        "def evaluate(model, X_train, Y_train, X_dev, Y_dev, X_test, Y_test, scale_factor):\n",
        "    scores = model.evaluate(X_train, Y_train, verbose = 0) * scale_factor * scale_factor\n",
        "    print(\"train: \", model.metrics_names, \": \", scores)\n",
        "    scores = model.evaluate(X_dev, Y_dev, verbose = 0) * scale_factor * scale_factor\n",
        "    print(\"dev: \", model.metrics_names, \": \", scores)\n",
        "    scores = model.evaluate(X_test, Y_test, verbose = 0) * scale_factor * scale_factor\n",
        "    print(\"test: \", model.metrics_names, \": \", scores)\n",
        "\n",
        "# Calculate MSE between two arrays of values\n",
        "def mse(predicted, observed):\n",
        "    return np.sum(np.multiply((predicted - observed),(predicted - observed)))/predicted.shape[0]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksH9aft26krC"
      },
      "source": [
        "def main():\n",
        "    #plt.switch_backend('tkAgg')\n",
        "\n",
        "    #Import test data (6027, 13)\n",
        "    train_dataframe = pandas.read_csv(DATASET_PATH + 'weather_train.csv', sep=\";\", engine='python', header = None)\n",
        "    dev_dataframe = pandas.read_csv(DATASET_PATH + 'weather_dev.csv', sep=\";\", engine='python', header = None)\n",
        "    test_dataframe = pandas.read_csv(DATASET_PATH + 'weather_test.csv', sep=\";\", engine='python', header = None)\n",
        "    train_data, train_labels, dev_data, dev_labels, test_data, test_labels, scale_factor = import_data(train_dataframe, dev_dataframe, test_dataframe)\n",
        "    from IPython.display import display\n",
        "    display(train_dataframe.head(10))\n",
        "\n",
        "\n",
        "    time_steps = 1\n",
        "    assert(train_data.shape[0] % time_steps == 0)\n",
        "\n",
        "    X_train = np.reshape(train_data, (train_data.shape[0] // time_steps, time_steps, train_data.shape[1]))\n",
        "    X_dev = np.reshape(dev_data, (dev_data.shape[0] // time_steps, time_steps, dev_data.shape[1]))\n",
        "    X_test = np.reshape(test_data, (test_data.shape[0] // time_steps, time_steps, test_data.shape[1]))\n",
        "    Y_train = np.reshape(train_labels, (train_labels.shape[0] // time_steps, time_steps, 1))\n",
        "    Y_dev = np.reshape(dev_labels, (dev_labels.shape[0] // time_steps, time_steps, 1))\n",
        "    Y_test = np.reshape(test_labels, (test_labels.shape[0] // time_steps, time_steps, 1))\n",
        "\n",
        "    model = build_model('glorot_uniform', 'adam')\n",
        "\n",
        "    #Standard vanilla LSTM model\n",
        "\n",
        "    model_fit_epochs = 100\n",
        "    print(\"X_train shape: \",X_train.shape, \" Y_train shape: \",Y_train.shape)\n",
        "\n",
        "    model.fit(\n",
        "        X_train, Y_train,\n",
        "        batch_size = 16, epochs = model_fit_epochs)\n",
        "    trainset_predicted = model.predict(X_train)\n",
        "    devset_predicted = model.predict(X_dev)\n",
        "    testset_predicted = model.predict(X_test)\n",
        "    \n",
        "    print(trainset_predicted.shape, Y_train.shape)\n",
        "    \n",
        "    print(\"Train r2_score: \", r2_score(trainset_predicted.reshape(-1, 1), Y_train.reshape(-1, 1)) )\n",
        "    print(\"Dev r2_score: \", r2_score(devset_predicted.reshape(-1, 1), Y_dev.reshape(-1, 1)) )\n",
        "    print(\"Test r2_score: \", r2_score(testset_predicted.reshape(-1, 1), Y_test.reshape(-1, 1)))\n",
        "\n",
        "    print(\"Train MSE: \", mse(trainset_predicted, Y_train) * scale_factor * scale_factor)\n",
        "    print(\"Dev MSE: \", mse(devset_predicted, Y_dev) * scale_factor * scale_factor)\n",
        "    print(\"Test MSE: \", mse(testset_predicted, Y_test) * scale_factor * scale_factor)\n",
        "\n",
        "    write_to_csv(trainset_predicted,'nn_trainset_prediction.csv')\n",
        "    write_to_csv(devset_predicted,'nn_devset_prediction.csv')\n",
        "    write_to_csv(testset_predicted, 'nn_testset_prediction.csv')\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YdyUmGUM6krD",
        "outputId": "e69e6d99-6e4f-4eed-8783-d2b0d0a07eb6"
      },
      "source": [
        "main()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>6.97</td>\n",
              "      <td>6.90</td>\n",
              "      <td>98.24</td>\n",
              "      <td>10.92</td>\n",
              "      <td>29.18</td>\n",
              "      <td>29.97</td>\n",
              "      <td>449.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>3.96</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>70.20</td>\n",
              "      <td>17.96</td>\n",
              "      <td>29.12</td>\n",
              "      <td>29.91</td>\n",
              "      <td>3165.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>2016</td>\n",
              "      <td>0.04</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2.99</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>71.04</td>\n",
              "      <td>10.57</td>\n",
              "      <td>29.37</td>\n",
              "      <td>30.17</td>\n",
              "      <td>2637.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>2017</td>\n",
              "      <td>1.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>21.60</td>\n",
              "      <td>17.14</td>\n",
              "      <td>72.80</td>\n",
              "      <td>11.84</td>\n",
              "      <td>29.53</td>\n",
              "      <td>30.33</td>\n",
              "      <td>710.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>27</td>\n",
              "      <td>2</td>\n",
              "      <td>2017</td>\n",
              "      <td>1.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2.75</td>\n",
              "      <td>0.70</td>\n",
              "      <td>85.40</td>\n",
              "      <td>6.64</td>\n",
              "      <td>29.37</td>\n",
              "      <td>30.17</td>\n",
              "      <td>680.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>25</td>\n",
              "      <td>5</td>\n",
              "      <td>2016</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>20.08</td>\n",
              "      <td>17.37</td>\n",
              "      <td>86.04</td>\n",
              "      <td>8.04</td>\n",
              "      <td>29.26</td>\n",
              "      <td>30.05</td>\n",
              "      <td>273.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>13</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>26.22</td>\n",
              "      <td>9.53</td>\n",
              "      <td>32.68</td>\n",
              "      <td>19.68</td>\n",
              "      <td>28.85</td>\n",
              "      <td>29.63</td>\n",
              "      <td>4692.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>23</td>\n",
              "      <td>6</td>\n",
              "      <td>2016</td>\n",
              "      <td>0.53</td>\n",
              "      <td>10.00</td>\n",
              "      <td>30.03</td>\n",
              "      <td>24.38</td>\n",
              "      <td>69.92</td>\n",
              "      <td>9.48</td>\n",
              "      <td>29.09</td>\n",
              "      <td>29.88</td>\n",
              "      <td>3000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>14</td>\n",
              "      <td>28</td>\n",
              "      <td>5</td>\n",
              "      <td>2017</td>\n",
              "      <td>0.73</td>\n",
              "      <td>10.00</td>\n",
              "      <td>25.62</td>\n",
              "      <td>16.38</td>\n",
              "      <td>55.28</td>\n",
              "      <td>17.20</td>\n",
              "      <td>28.97</td>\n",
              "      <td>29.76</td>\n",
              "      <td>3040.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>2016</td>\n",
              "      <td>0.45</td>\n",
              "      <td>7.97</td>\n",
              "      <td>10.10</td>\n",
              "      <td>8.58</td>\n",
              "      <td>88.27</td>\n",
              "      <td>10.18</td>\n",
              "      <td>29.10</td>\n",
              "      <td>29.89</td>\n",
              "      <td>297.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0   1   2     3     4      5   ...     7      8      9      10     11       12\n",
              "0  12   2   1  2017  1.00   1.85  ...   6.90  98.24  10.92  29.18  29.97   449.25\n",
              "1   9   7   2  2016  0.00  10.00  ...  -0.61  70.20  17.96  29.12  29.91  3165.50\n",
              "2   9  22   2  2016  0.04  10.00  ...  -1.34  71.04  10.57  29.37  30.17  2637.00\n",
              "3   8   3  10  2017  1.00  10.00  ...  17.14  72.80  11.84  29.53  30.33   710.25\n",
              "4   7  27   2  2017  1.00  10.00  ...   0.70  85.40   6.64  29.37  30.17   680.50\n",
              "5   6  25   5  2016  0.00  10.00  ...  17.37  86.04   8.04  29.26  30.05   273.95\n",
              "6  13  25   4  2017  0.00  10.00  ...   9.53  32.68  19.68  28.85  29.63  4692.00\n",
              "7  10  23   6  2016  0.53  10.00  ...  24.38  69.92   9.48  29.09  29.88  3000.00\n",
              "8  14  28   5  2017  0.73  10.00  ...  16.38  55.28  17.20  28.97  29.76  3040.50\n",
              "9   7  11   4  2016  0.45   7.97  ...   8.58  88.27  10.18  29.10  29.89   297.75\n",
              "\n",
              "[10 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (6028, 1, 12)  Y_train shape:  (6028, 1, 1)\n",
            "Epoch 1/100\n",
            "377/377 [==============================] - 6s 4ms/step - loss: 0.0998\n",
            "Epoch 2/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0649\n",
            "Epoch 3/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0576\n",
            "Epoch 4/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0549\n",
            "Epoch 5/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0519\n",
            "Epoch 6/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0490\n",
            "Epoch 7/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0442\n",
            "Epoch 8/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0389\n",
            "Epoch 9/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0364\n",
            "Epoch 10/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0354\n",
            "Epoch 11/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0356\n",
            "Epoch 12/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0345\n",
            "Epoch 13/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0341\n",
            "Epoch 14/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0333\n",
            "Epoch 15/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0327\n",
            "Epoch 16/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0315\n",
            "Epoch 17/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0326\n",
            "Epoch 18/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0321\n",
            "Epoch 19/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0312\n",
            "Epoch 20/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0310\n",
            "Epoch 21/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0321\n",
            "Epoch 22/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0304\n",
            "Epoch 23/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0320\n",
            "Epoch 24/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0296\n",
            "Epoch 25/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0310\n",
            "Epoch 26/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0323\n",
            "Epoch 27/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0317\n",
            "Epoch 28/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0303\n",
            "Epoch 29/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0296\n",
            "Epoch 30/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0294\n",
            "Epoch 31/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0294\n",
            "Epoch 32/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0300\n",
            "Epoch 33/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0302\n",
            "Epoch 34/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0285\n",
            "Epoch 35/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0286\n",
            "Epoch 36/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0297\n",
            "Epoch 37/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0300\n",
            "Epoch 38/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0284\n",
            "Epoch 39/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0316\n",
            "Epoch 40/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0299\n",
            "Epoch 41/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0298\n",
            "Epoch 42/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0301\n",
            "Epoch 43/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0302\n",
            "Epoch 44/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0292\n",
            "Epoch 45/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0280\n",
            "Epoch 46/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0292\n",
            "Epoch 47/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0294\n",
            "Epoch 48/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0294\n",
            "Epoch 49/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0284\n",
            "Epoch 50/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0294\n",
            "Epoch 51/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0295\n",
            "Epoch 52/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0296\n",
            "Epoch 53/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0306\n",
            "Epoch 54/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0290\n",
            "Epoch 55/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0285\n",
            "Epoch 56/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0280\n",
            "Epoch 57/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0297\n",
            "Epoch 58/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0294\n",
            "Epoch 59/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0284\n",
            "Epoch 60/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0296\n",
            "Epoch 61/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0290\n",
            "Epoch 62/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0290\n",
            "Epoch 63/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0288\n",
            "Epoch 64/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0281\n",
            "Epoch 65/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0272\n",
            "Epoch 66/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0286\n",
            "Epoch 67/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0283\n",
            "Epoch 68/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0286\n",
            "Epoch 69/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0281\n",
            "Epoch 70/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0285\n",
            "Epoch 71/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0288\n",
            "Epoch 72/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0287\n",
            "Epoch 73/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0284\n",
            "Epoch 74/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0296\n",
            "Epoch 75/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0286\n",
            "Epoch 76/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0276\n",
            "Epoch 77/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0286\n",
            "Epoch 78/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0276\n",
            "Epoch 79/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0286\n",
            "Epoch 80/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0268\n",
            "Epoch 81/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0282\n",
            "Epoch 82/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0276\n",
            "Epoch 83/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0283\n",
            "Epoch 84/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0269\n",
            "Epoch 85/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0287\n",
            "Epoch 86/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0277\n",
            "Epoch 87/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0282\n",
            "Epoch 88/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0288\n",
            "Epoch 89/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0267\n",
            "Epoch 90/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0276\n",
            "Epoch 91/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0269\n",
            "Epoch 92/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0264\n",
            "Epoch 93/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0276\n",
            "Epoch 94/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0272\n",
            "Epoch 95/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0280\n",
            "Epoch 96/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0282\n",
            "Epoch 97/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0274\n",
            "Epoch 98/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0274\n",
            "Epoch 99/100\n",
            "377/377 [==============================] - 1s 4ms/step - loss: 0.0280\n",
            "Epoch 100/100\n",
            "377/377 [==============================] - 2s 4ms/step - loss: 0.0282\n",
            "(6028, 1, 1) (6028, 1, 1)\n",
            "Train r2_score:  0.6701755938115942\n",
            "Dev r2_score:  0.7150115026803614\n",
            "Test r2_score:  0.6714691562963172\n",
            "Train MSE:  538878.3656905469\n",
            "Dev MSE:  482808.274346918\n",
            "Test MSE:  538721.4723510331\n",
            "Writing to CSV...\n",
            "...finished!\n",
            "Writing to CSV...\n",
            "...finished!\n",
            "Writing to CSV...\n",
            "...finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMOcBleGCXbH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}